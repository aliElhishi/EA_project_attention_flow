{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mattention_graph_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from attention_graph_util import *\n",
    "import seaborn as sns\n",
    "import itertools \n",
    "import matplotlib as mpl\n",
    "import networkx as nx\n",
    "import os\n",
    "from util import constants\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import pandas as pd\n",
    "\n",
    "from util.models import MODELS\n",
    "from util.tasks import TASKS\n",
    "#from dnotebook_utils import *\n",
    "from attention_graph_util import *\n",
    "%matplotlib inline\n",
    "from util.config_util import get_task_params\n",
    "from notebooks.notebook_utils import *\n",
    "from util import inflect\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "import math\n",
    "\n",
    "\n",
    "rc={'font.size': 10, 'axes.labelsize': 10, 'legend.fontsize': 10.0, \n",
    "    'axes.titlesize': 32, 'xtick.labelsize': 20, 'ytick.labelsize': 16}\n",
    "plt.rcParams.update(**rc)\n",
    "mpl.rcParams['axes.linewidth'] = .5 #set the value globally\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "from transformers import BertConfig, BertForMaskedLM, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\python311\\lib\\site-packages (4.39.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in c:\\python311\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.29.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python311\\lib\\site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python311\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python311\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\python311\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\python311\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.8/10.0 MB 2.8 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.6/10.0 MB 2.7 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.8/10.0 MB 2.6 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.4/10.0 MB 2.6 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.9/10.0 MB 2.5 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 3.4/10.0 MB 2.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.9/10.0 MB 2.4 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 4.5/10.0 MB 2.4 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 4.7/10.0 MB 2.4 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 5.5/10.0 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.0/10.0 MB 2.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.6/10.0 MB 2.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.1/10.0 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 7.6/10.0 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.1/10.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.7/10.0 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.2/10.0 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.0 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 2.5 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.29.2-py3-none-any.whl (468 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.4 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.4 MB 1.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 1.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.3/2.4 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.22.2\n",
      "    Uninstalling huggingface-hub-0.22.2:\n",
      "      Successfully uninstalled huggingface-hub-0.22.2\n",
      "  Rolling back uninstall of huggingface-hub\n",
      "  Moving to c:\\users\\aliel\\appdata\\roaming\\python\\python311\\scripts\\huggingface-cli.exe\n",
      "   from C:\\Users\\aliel\\AppData\\Local\\Temp\\pip-uninstall-pa8pim3v\\huggingface-cli.exe\n",
      "  Moving to c:\\users\\aliel\\appdata\\roaming\\python\\python311\\site-packages\\huggingface_hub-0.22.2.dist-info\\\n",
      "   from C:\\Users\\aliel\\AppData\\Roaming\\Python\\Python311\\site-packages\\~uggingface_hub-0.22.2.dist-info\n",
      "  Moving to c:\\users\\aliel\\appdata\\roaming\\python\\python311\\site-packages\\huggingface_hub\\\n",
      "   from C:\\Users\\aliel\\AppData\\Roaming\\Python\\Python311\\site-packages\\~uggingface_hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'C:\\\\Python311\\\\Scripts\\\\huggingface-cli.exe' -> 'C:\\\\Python311\\\\Scripts\\\\huggingface-cli.exe.deleteme'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: networkx in c:\\python311\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: matplotlib in c:\\python311\\lib\\site-packages (3.8.2)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\python311\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python311\\lib\\site-packages (from matplotlib) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\python311\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\python311\\lib\\site-packages (from matplotlib) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python311\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\python311\\lib\\site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python311\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\python311\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.10.1-cp311-cp311-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.8/8.1 MB 2.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.3/8.1 MB 2.2 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.8/8.1 MB 2.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.4/8.1 MB 2.4 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.9/8.1 MB 2.5 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.4/8.1 MB 2.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.2/8.1 MB 2.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.7/8.1 MB 2.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.2/8.1 MB 2.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 6.0/8.1 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.6/8.1 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.1/8.1 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.1 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 2.6 MB/s eta 0:00:00\n",
      "Installing collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.8.2\n",
      "    Uninstalling matplotlib-3.8.2:\n",
      "      Successfully uninstalled matplotlib-3.8.2\n",
      "Successfully installed matplotlib-3.10.1\n",
      "Requirement already satisfied: seaborn in c:\\python311\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\python311\\lib\\site-packages (from seaborn) (1.26.2)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\python311\\lib\\site-packages (from seaborn) (2.2.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\python311\\lib\\site-packages (from seaborn) (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\python311\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python311\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python311\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Requirement already satisfied: torch in c:\\python311\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\python311\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: filelock in c:\\python311\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\python311\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: networkx in c:\\python311\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\python311\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\python311\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\python311\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python311\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\python311\\lib\\site-packages (from torchvision) (1.26.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\python311\\lib\\site-packages (from torchvision) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python311\\lib\\site-packages (from jinja2->torch) (2.1.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip install networkx\n",
    "!pip install --upgrade matplotlib\n",
    "!pip install --upgrade seaborn\n",
    "\n",
    "\n",
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Constructing tf.data.Dataset for split validation, from ../InDist/data/word_sv_agreement/0.1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab len:  10032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Constructing tf.data.Dataset for split test, from ../InDist/data/word_sv_agreement/0.1.0\n",
      "INFO:absl:Constructing tf.data.Dataset for split train, from ../InDist/data/word_sv_agreement/0.1.0\n"
     ]
    }
   ],
   "source": [
    "task_name = 'word_sv_agreement_lm'\n",
    "task_params = get_task_params(batch_size=1)\n",
    "task = TASKS[task_name](task_params, data_dir='../InDist/data')\n",
    "cl_token = task.sentence_encoder().encode(constants.bos)\n",
    "task_tokenizer = task.sentence_encoder()._tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2d2392a0eb447b82262feec37ae995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased',\n",
    "                                        output_hidden_states=True,\n",
    "                                        output_attentions=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_convertor(encoded_input_task, task_offset, task_encoder, tokenizer):\n",
    "    string_part1 = task_encoder.decode(encoded_input_task[:task_offset])\n",
    "    tokens_part1 = tokenizer.tokenize(string_part1)\n",
    "    \n",
    "    return len(tokens_part1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "many NNS of woodland remain and support a JJ sector in the southern portion of the state .\n",
      "21 ['cls', 'many', 'n', '##ns', 'of', 'woodland', 'remain', 'and', 'support', 'a', 'jj', 'sector', 'in', 'the', 'southern', 'portion', 'of', 'the', 'state', '.', 'sep']\n",
      "torch.Size([1, 21, 30522])\n",
      "(6, 12, 21, 21)\n",
      "torch.Size([1, 21, 768])\n",
      "tensor(-3811819.5000, grad_fn=<SumBackward0>)\n",
      "torch.Size([1, 21, 768])\n"
     ]
    }
   ],
   "source": [
    "for x,y in task.test_dataset:\n",
    "    sentence = task.sentence_encoder().decode(x[0][1:])\n",
    "    print(sentence)\n",
    "    break\n",
    "\n",
    "tokens = ['cls']+tokenizer.tokenize(sentence)+['sep']\n",
    "print(len(tokens), tokens)\n",
    "tf_input_ids = tokenizer.encode(sentence)\n",
    "input_ids = torch.tensor([tf_input_ids])\n",
    "logits, all_hidden_states, all_attentions = model(input_ids)\n",
    "print(logits.shape)\n",
    "_attentions = [att.detach().numpy() for att in all_attentions]\n",
    "attentions_mat = np.asarray(_attentions)[:,0]\n",
    "print(attentions_mat.shape)\n",
    "\n",
    "embeded_inputs = torch.autograd.Variable(model.distilbert.embeddings(input_ids), requires_grad=True)\n",
    "logits, all_hidden_states, all_attentions = model(inputs_embeds=embeded_inputs)\n",
    "print(embeded_inputs.shape)\n",
    "\n",
    "\n",
    "lsum = logits.sum()\n",
    "print(lsum)\n",
    "\n",
    "lsum.backward()\n",
    "embeded_inputs.require_grad = True\n",
    "print(embeded_inputs.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Constructing tf.data.Dataset for split validation, from ../InDist/data/word_sv_agreement/0.1.0\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:00,  5.31it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'ready', 'to', 'serve', 'at', 'every', 'opportunity', ',', 'yet', 'making', 'sure', 'that', 'your', 'fellow', 'servers', '[MASK]', 'an', 'equal', 'chance', '.', '[SEP]']\n",
      "['[CLS]', 'reviewed', 'journals', '[MASK]', 'of', 'varying', 'degrees', 'of', 'reliability', '.', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "3it [00:00,  6.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "4it [00:00,  5.60it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'operation', 'since', '1871', ',', 'the', 'network', '[MASK]', 'presently', 'about', 'long', ',', 'and', 'comprises', '10', 'lines', '.', '[SEP]']\n",
      "['[CLS]', 'peak', 'times', ',', 'the', 'n', '##np', 'route', '[MASK]', 'via', 'the', 'n', '##np', 'guided', 'n', '##np', 'to', 'cambridge', '.', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "5it [00:00,  5.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "8it [00:01,  6.47it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'first', 'requirement', '(', 'n', '##np', ')', 'simply', '[MASK]', 'that', 'a', 'cd', 'should', 'be', 'a', 'distribution', 'on', 'the', 'parameter', 'space', '.', '[SEP]']\n",
      "['[CLS]', 'letters', '[MASK]', 'small', 'and', 'jj', '.', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "10it [00:01,  6.80it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'women', 'the', 'number', '[MASK]', 'one', 'in', 'forty', 'and', 'the', 'n', '##ns', 'are', 'more', 'likely', 'to', 'be', 'prison', 'staff', 'members', '.', '[SEP]']\n",
      "['[CLS]', 'n', '##n', 'link', '[MASK]', 'because', 'the', 'company', 'uses', 'n', '##n', 'products', '.', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "12it [00:01,  7.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "13it [00:01,  7.06it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'support', 'he', '[MASK]', 'a', 'fine', 'editor', ',', 'but', 'has', 'too', 'little', 'edit', '##s', '.', '[SEP]']\n",
      "['[CLS]', 'method', '[MASK]', 'the', 'magnetic', 'n', '##n', 'that', 'the', 'n', '##n', 'experiences', ',', 'constant', 'over', 'the', 'n', '##n', \"'\", 's', 'normal', 'n', '##n', 'range', '.', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:01,  7.00it/s]\n"
     ]
    }
   ],
   "source": [
    "all_examples_x = []\n",
    "all_examples_vp = []\n",
    "all_examples_y = []\n",
    "\n",
    "all_examples_attentions = []\n",
    "all_examples_blankout_relevance = []\n",
    "all_examples_grads = []\n",
    "all_examples_inputgrads = []\n",
    "n_batches = 10\n",
    "\n",
    "all_examples_accuracies = []\n",
    "\n",
    "infl_eng = inflect.engine()\n",
    "verb_infl, noun_infl = gen_inflect_from_vocab(infl_eng, '../InDist/notebooks/wiki.vocab')\n",
    "\n",
    "test_data = task.databuilder.as_dataset(split='validation', batch_size=1)\n",
    "for examples in tqdm(test_data):\n",
    "    sentence = task.sentence_encoder().decode(examples['sentence'][0][1:])\n",
    "    if len(examples['sentence'][0][1:]) > 20:\n",
    "        continue\n",
    "    \n",
    "    verb_position = examples['verb_position'][0].numpy()-1  #+1 because of adding cls.\n",
    "    verb_position = offset_convertor(examples['sentence'][0], verb_position, task.sentence_encoder(), tokenizer)\n",
    "    \n",
    "    sentence = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    all_examples_vp.append(verb_position)\n",
    "    sentence[verb_position] = tokenizer.mask_token\n",
    "    \n",
    "    tf_input_ids = tokenizer.encode(sentence)\n",
    "    input_ids = torch.tensor([tf_input_ids])\n",
    "    sentence = tokenizer.tokenize(tokenizer.decode(tf_input_ids))\n",
    "    print(sentence)\n",
    "\n",
    "    \n",
    "    s_shape = input_ids.shape\n",
    "    batch_size, length = s_shape[0], s_shape[1]\n",
    "    actual_verb = examples['verb'][0].numpy().decode(\"utf-8\")\n",
    "    inflected_verb = verb_infl[actual_verb] \n",
    "\n",
    "\n",
    "    actual_verb_index = tokenizer.encode(tokenizer.tokenize(actual_verb))[1]\n",
    "    inflected_verb_index = tokenizer.encode(tokenizer.tokenize(inflected_verb))[1]\n",
    "\n",
    "    all_examples_x.append(input_ids)\n",
    "    embeded_inputs = torch.autograd.Variable(model.distilbert.embeddings(input_ids), requires_grad=True)\n",
    "    predictions = model(inputs_embeds=embeded_inputs)\n",
    "    logits = predictions[0][0]\n",
    "\n",
    "    \n",
    "        \n",
    "    probs = torch.nn.Softmax(dim=-1)(logits)\n",
    "    actual_verb_score = probs[verb_position][actual_verb_index]\n",
    "    inflected_verb_score = probs[verb_position][inflected_verb_index]\n",
    "    \n",
    "    main_diff_score = actual_verb_score - inflected_verb_score\n",
    "    \n",
    "    all_examples_accuracies.append(main_diff_score > 0)\n",
    "    \n",
    "    logits_sum = logits.sum()\n",
    "    actual_verb_score.backward()\n",
    "    grads = embeded_inputs.grad\n",
    "    grad_scores = abs(np.sum(grads.detach().numpy(), axis=-1))\n",
    "    input_grad_scores = abs(np.sum((grads * embeded_inputs).detach().numpy(), axis=-1))\n",
    "    all_examples_grads.append(grad_scores)\n",
    "    all_examples_inputgrads.append(input_grad_scores)\n",
    "    \n",
    "    hidden_states, attentions = predictions[-2:]\n",
    "    _attentions = [att.detach().numpy() for att in attentions]\n",
    "    attentions_mat = np.asarray(_attentions)[:,0]\n",
    "\n",
    "    all_examples_attentions.append(attentions_mat)\n",
    "    \n",
    "    n_batches -= 1\n",
    "    if n_batches <= 0:\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_att_relevance(full_att_mat, input_tokens, layer=-1, output_index=0):\n",
    "    raw_rel = full_att_mat[layer].sum(axis=0)[output_index]/full_att_mat[layer].sum(axis=0)[output_index].sum()\n",
    "    \n",
    "    return raw_rel\n",
    "\n",
    "\n",
    "def get_joint_relevance(full_att_mat, input_tokens, layer=-1, output_index=0):\n",
    "    att_sum_heads =  full_att_mat.sum(axis=1) / full_att_mat.shape[1]\n",
    "    joint_attentions = compute_joint_attention(att_sum_heads, add_residual=True)\n",
    "    relevance_attentions = joint_attentions[layer][output_index]\n",
    "    return relevance_attentions\n",
    "\n",
    "\n",
    "def get_flow_relevance(full_att_mat, input_tokens, layer, output_index):\n",
    "    \n",
    "    input_tokens = input_tokens\n",
    "    res_att_mat = full_att_mat.sum(axis=1)/full_att_mat.shape[1]\n",
    "    res_att_mat = res_att_mat + np.eye(res_att_mat.shape[1])[None,...]\n",
    "    res_att_mat = res_att_mat / res_att_mat.sum(axis=-1)[...,None]\n",
    "\n",
    "    res_adj_mat, res_labels_to_index = get_adjmat(mat=res_att_mat, input_tokens=input_tokens)\n",
    "    \n",
    "    A = res_adj_mat\n",
    "    res_G=nx.from_numpy_matrix(A, create_using=nx.DiGraph())\n",
    "    for i in np.arange(A.shape[0]):\n",
    "        for j in np.arange(A.shape[1]):\n",
    "            nx.set_edge_attributes(res_G, {(i,j): A[i,j]}, 'capacity')\n",
    "\n",
    "\n",
    "    output_nodes = ['L'+str(layer+1)+'_'+str(output_index)]\n",
    "    input_nodes = []\n",
    "    for key in res_labels_to_index:\n",
    "        if res_labels_to_index[key] < full_att_mat.shape[-1]:\n",
    "            input_nodes.append(key)\n",
    "    \n",
    "    flow_values = compute_node_flow(res_G, res_labels_to_index, input_nodes=input_nodes, output_nodes=output_nodes, length=full_att_mat.shape[-1])\n",
    "    \n",
    "    n_layers = full_att_mat.shape[0]\n",
    "    length = full_att_mat.shape[-1]\n",
    "    final_layer_attention = flow_values[(layer+1)*length:, layer*length:(layer+1)*length]\n",
    "    relevance_attention_flow = final_layer_attention[output_index]\n",
    "\n",
    "    return relevance_attention_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 12, 21, 21)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_examples_attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 2236.84it/s]\n",
      "\n",
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 1624.63it/s]\n",
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute raw relevance scores ...\n",
      "compute joint relevance scores ...\n",
      "compute flow relevance scores ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:03<00:32,  3.65s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 2/10 [00:04<00:21,  2.67s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 3/10 [00:05<00:16,  2.42s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 4/10 [00:08<00:15,  2.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 5/10 [00:13<00:15,  3.15s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 7/10 [00:17<00:08,  2.81s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 8/10 [00:18<00:04,  2.21s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 9/10 [00:19<00:01,  1.89s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 10/10 [00:27<00:00,  2.78s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "print(\"compute raw relevance scores ...\")\n",
    "all_examples_raw_relevance = {}\n",
    "for l in np.arange(5,6):\n",
    "    all_examples_raw_relevance[l] = []\n",
    "    for i in tqdm(np.arange(len(all_examples_x))):\n",
    "        tokens = tokenizer.tokenize(tokenizer.decode(all_examples_x[i][0].numpy()))\n",
    "        vp = all_examples_vp[i]\n",
    "        length = len(tokens)\n",
    "        attention_relevance = get_raw_att_relevance(all_examples_attentions[i], tokens, layer=l, output_index=vp)\n",
    "        all_examples_raw_relevance[l].append(np.asarray(attention_relevance))\n",
    "\n",
    "print(\"compute joint relevance scores ...\")\n",
    "all_examples_joint_relevance = {}\n",
    "for l in np.arange(5,6):\n",
    "    all_examples_joint_relevance[l] = []\n",
    "    for i in tqdm(np.arange(len(all_examples_x))):\n",
    "        tokens = tokenizer.tokenize(tokenizer.decode(all_examples_x[i][0].numpy()))\n",
    "        vp = all_examples_vp[i]\n",
    "        length = len(tokens)\n",
    "        attention_relevance = get_joint_relevance(all_examples_attentions[i], tokens, layer=l, output_index=vp)\n",
    "        all_examples_joint_relevance[l].append(np.asarray(attention_relevance))\n",
    "    \n",
    "print(\"compute flow relevance scores ...\")\n",
    "all_examples_flow_relevance = {}\n",
    "for l in np.arange(5,6):\n",
    "    all_examples_flow_relevance[l] = []\n",
    "    for i in tqdm(np.arange(len(all_examples_x))):\n",
    "        tokens = tokenizer.tokenize(tokenizer.decode(all_examples_x[i][0].numpy()))\n",
    "        vp = all_examples_vp[i]\n",
    "        length = len(tokens)\n",
    "        attention_relevance = get_flow_relevance(all_examples_attentions[i], tokens, layer=l, output_index=vp)\n",
    "        all_examples_flow_relevance[l].append(np.asarray(attention_relevance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############Layer  5 #############\n",
      "raw grad\n",
      "(21,) (21,)\n",
      "0.15722765381669673 0.3068715774066908\n",
      "joint grad\n",
      "(21,) (21,)\n",
      "0.2847923704897725 0.26784481005012173\n",
      "flow grad\n",
      "(21,) (21,)\n",
      "0.278563534317075\n"
     ]
    }
   ],
   "source": [
    "for l in np.arange(5,6):\n",
    "    print(\"###############Layer \",l, \"#############\")\n",
    "\n",
    "    print('raw grad')\n",
    "    print(all_examples_raw_relevance[l][0].shape, all_examples_grads[0][0].shape)\n",
    "    raw_sps_grad = []\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = spearmanr(all_examples_raw_relevance[l][i],all_examples_grads[i][0])\n",
    "        if not math.isnan(sp[0]):\n",
    "            raw_sps_grad.append(sp[0])\n",
    "        else:\n",
    "            raw_sps_grad.append(0)\n",
    "        \n",
    "    print(np.mean(raw_sps_grad), np.std(raw_sps_grad))\n",
    "\n",
    "    \n",
    "    print('joint grad')\n",
    "    print(all_examples_joint_relevance[l][0].shape, all_examples_grads[0][0].shape)\n",
    "    joint_sps_grad = []\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = spearmanr(all_examples_joint_relevance[l][i],all_examples_grads[i][0])\n",
    "        if not math.isnan(sp[0]):\n",
    "            joint_sps_grad.append(sp[0])\n",
    "        else:\n",
    "            joint_sps_grad.append(0)\n",
    "        \n",
    "    print(np.mean(joint_sps_grad), np.std(joint_sps_grad))\n",
    "\n",
    "  \n",
    "    print('flow grad')\n",
    "    print(all_examples_joint_relevance[l][0].shape, all_examples_grads[0][0].shape)\n",
    "    flow_sps_grad = []\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = spearmanr(all_examples_flow_relevance[l][i],all_examples_grads[i][0])\n",
    "        if not math.isnan(sp[0]):\n",
    "            flow_sps_grad.append(sp[0])\n",
    "        else:\n",
    "            flow_sps_grad.append(0)\n",
    "        \n",
    "    print(np.mean(flow_sps_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_examples_joint_relevance[l][0].shape)\n",
    "print(all_examples_flow_relevance[l][0].shape)\n",
    "print(all_examples_blankout_relevance[0].numpy().shape)\n",
    "print(all_examples_inputgrads[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_examples_inputgrads[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = model\n",
    "\n",
    "sentences = []\n",
    "all_atts = []\n",
    "all_main_probs = []\n",
    "all_index_probs = []\n",
    "all_gradient_scores = []\n",
    "all_inputgradient_scores = []\n",
    "prob_fn = task.get_probs_fn()\n",
    "count = 0\n",
    "for x, y in task.test_dataset:\n",
    "    \n",
    "     #Manually add cls token:\n",
    "    batch_size = len(x)\n",
    "    cl_token = tf.reshape(tf.convert_to_tensor(cl_token[0], dtype=tf.int64)[None], (-1,1))\n",
    "    cl_tokens = tf.tile(cl_token, (batch_size, 1))\n",
    "    x = tf.concat([cl_tokens, x], axis=-1)\n",
    "    \n",
    "    # Get gradient scores \n",
    "    input_embeddings, input_shape, padding_mask, past = model.get_input_embeddings(x, training=False, add_cls=False)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_embeddings)\n",
    "        outputs = model_1.call_with_embeddings(input_embeddings, input_shape, padding_mask, past)\n",
    "        logits = outputs[0]\n",
    "        probs = tf.nn.softmax(logits, axis=-1)\n",
    "        diff_probs = probs[:,0] - probs[:,1]\n",
    "        \n",
    "    grads = tape.gradient(diff_probs, input_embeddings)\n",
    "    grad_scores = tf.abs(tf.reduce_sum(grads, axis=-1))\n",
    "    input_grad_scores = tf.abs(tf.reduce_sum(tf.multiply(grads, input_embeddings), axis=-1))\n",
    "    \n",
    "    \n",
    "    all_gradient_scores.extend(grad_scores)\n",
    "    all_inputgradient_scores.extend(input_grad_scores)\n",
    "    \n",
    "    \n",
    "    max_len = x.shape[1]\n",
    "    all_outputs = model_1.detailed_call(x, training=False, add_cls=False)\n",
    "    main_logits = all_outputs[0]\n",
    "    attentions = all_outputs[6]\n",
    "    _attentions = [att.numpy() for att in attentions]\n",
    "    attentions = np.transpose(np.asarray(_attentions), (1,0,2,3,4))\n",
    "    main_probs = prob_fn(main_logits, y, 1)\n",
    "    batch_indexes = tf.range(len(y), dtype=tf.int64)\n",
    "    indexes = tf.concat([batch_indexes[:,None], y[:,None]], axis=1)\n",
    "    correct_main_probs = tf.gather_nd(main_probs, indexes).numpy()\n",
    "\n",
    "    sentences.append(task.databuilder.sentence_encoder().decode(x[0]))\n",
    "    all_atts.extend(attentions)\n",
    "    all_main_probs.extend(correct_main_probs)\n",
    "    all_index_probs.append([])\n",
    "    \n",
    "    # This loop can be optimized so that there is only one call...\n",
    "    new_xz = []\n",
    "    for i in np.arange(0,max_len):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        unktoken = task.databuilder.sentence_encoder().encode(constants.unk)\n",
    "        unk = tf.reshape(tf.convert_to_tensor(unktoken, dtype=tf.int64)[None], (-1,1))\n",
    "        unks = tf.tile(unk, (batch_size, 1))\n",
    "        new_x = tf.concat([x[:,:i], unks, x[:,i+1:]], axis=-1)\n",
    "        new_xz.extend(new_x)\n",
    "    \n",
    "    new_x = np.asarray(new_xz)\n",
    "    logits = model_1(new_x, training=False, add_cls=False)\n",
    "    probs = prob_fn(logits, y, 1)\n",
    "    \n",
    "    batch_indexes = tf.range(len(probs), dtype=tf.int64)\n",
    "    yz = tf.tile(y, (len(probs),))\n",
    "\n",
    "    indexes = tf.concat([batch_indexes[:,None], yz[:,None]], axis=1)\n",
    "    \n",
    "    correct_probs = tf.gather_nd(probs, indexes).numpy()\n",
    "    all_index_probs[-1].extend(abs(correct_main_probs - correct_probs))\n",
    "    count += 1\n",
    "    if count > 100:\n",
    "        break\n",
    "    print (count, end=\"\\r\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
