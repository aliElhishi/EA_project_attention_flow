{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dehghani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from attention_graph_util import *\n",
    "import seaborn as sns\n",
    "import itertools \n",
    "import matplotlib as mpl\n",
    "import networkx as nx\n",
    "import os\n",
    "from util import constants\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import pandas as pd\n",
    "\n",
    "from util.models import MODELS\n",
    "from util.tasks import TASKS\n",
    "#from dnotebook_utils import *\n",
    "from attention_graph_util import *\n",
    "%matplotlib inline\n",
    "from util.config_util import get_task_params\n",
    "from notebooks.notebook_utils import *\n",
    "from util import inflect\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "import math\n",
    "\n",
    "\n",
    "rc={'font.size': 10, 'axes.labelsize': 10, 'legend.fontsize': 10.0, \n",
    "    'axes.titlesize': 32, 'xtick.labelsize': 20, 'ytick.labelsize': 16}\n",
    "plt.rcParams.update(**rc)\n",
    "mpl.rcParams['axes.linewidth'] = .5 #set the value globally\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "from transformers import BertConfig, BertForMaskedLM, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip install networkx\n",
    "!pip install --upgrade matplotlib\n",
    "!pip install --upgrade seaborn\n",
    "\n",
    "\n",
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Constructing tf.data.Dataset for split validation, from ../InDist/data/word_sv_agreement/0.1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab len:  10032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Constructing tf.data.Dataset for split test, from ../InDist/data/word_sv_agreement/0.1.0\n",
      "INFO:absl:Constructing tf.data.Dataset for split train, from ../InDist/data/word_sv_agreement/0.1.0\n"
     ]
    }
   ],
   "source": [
    "task_name = 'word_sv_agreement_lm'\n",
    "task_params = get_task_params(batch_size=1)\n",
    "task = TASKS[task_name](task_params, data_dir='../InDist/data')\n",
    "cl_token = task.sentence_encoder().encode(constants.bos)\n",
    "task_tokenizer = task.sentence_encoder()._tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2d2392a0eb447b82262feec37ae995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased',\n",
    "                                        output_hidden_states=True,\n",
    "                                        output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_convertor(encoded_input_task, task_offset, task_encoder, tokenizer):\n",
    "    string_part1 = task_encoder.decode(encoded_input_task[:task_offset])\n",
    "    tokens_part1 = tokenizer.tokenize(string_part1)\n",
    "    \n",
    "    return len(tokens_part1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "many NNS of woodland remain and support a JJ sector in the southern portion of the state .\n",
      "21 ['cls', 'many', 'n', '##ns', 'of', 'woodland', 'remain', 'and', 'support', 'a', 'jj', 'sector', 'in', 'the', 'southern', 'portion', 'of', 'the', 'state', '.', 'sep']\n",
      "torch.Size([1, 21, 30522])\n",
      "(6, 12, 21, 21)\n",
      "torch.Size([1, 21, 768])\n",
      "tensor(-3811819.5000, grad_fn=<SumBackward0>)\n",
      "torch.Size([1, 21, 768])\n"
     ]
    }
   ],
   "source": [
    "for x,y in task.test_dataset:\n",
    "    sentence = task.sentence_encoder().decode(x[0][1:])\n",
    "    print(sentence)\n",
    "    break\n",
    "\n",
    "tokens = ['cls']+tokenizer.tokenize(sentence)+['sep']\n",
    "print(len(tokens), tokens)\n",
    "tf_input_ids = tokenizer.encode(sentence)\n",
    "input_ids = torch.tensor([tf_input_ids])\n",
    "logits, all_hidden_states, all_attentions = model(input_ids)\n",
    "print(logits.shape)\n",
    "_attentions = [att.detach().numpy() for att in all_attentions]\n",
    "attentions_mat = np.asarray(_attentions)[:,0]\n",
    "print(attentions_mat.shape)\n",
    "\n",
    "embeded_inputs = torch.autograd.Variable(model.distilbert.embeddings(input_ids), requires_grad=True)\n",
    "logits, all_hidden_states, all_attentions = model(inputs_embeds=embeded_inputs)\n",
    "print(embeded_inputs.shape)\n",
    "\n",
    "\n",
    "lsum = logits.sum()\n",
    "print(lsum)\n",
    "\n",
    "lsum.backward()\n",
    "embeded_inputs.require_grad = True\n",
    "print(embeded_inputs.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Constructing tf.data.Dataset for split validation, from ../InDist/data/word_sv_agreement/0.1.0\n",
      "1it [00:00,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'ready', 'to', 'serve', 'at', 'every', 'opportunity', ',', 'yet', 'making', 'sure', 'that', 'your', 'fellow', 'servers', 'have', '[MASK]', 'equal', 'chance', '.', 'SEP']\n",
      "['[CLS]', 'reviewed', 'journals', 'are', '[MASK]', 'varying', 'degrees', 'of', 'reliability', '.', 'SEP']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'operation', 'since', '1871', ',', 'the', 'network', 'is', '[MASK]', 'about', 'long', ',', 'and', 'comprises', '10', 'lines', '.', 'SEP']\n",
      "['[CLS]', 'peak', 'times', ',', 'the', 'n', '##np', 'route', 'continues', '[MASK]', 'the', 'n', '##np', 'guided', 'n', '##np', 'to', 'cambridge', '.', 'SEP']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:01,  6.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'first', 'requirement', '(', 'n', '##np', ')', 'simply', 'requires', '[MASK]', 'a', 'cd', 'should', 'be', 'a', 'distribution', 'on', 'the', 'parameter', 'space', '.', 'SEP']\n",
      "['[CLS]', 'letters', 'are', '[MASK]', 'and', 'jj', '.', 'SEP']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:01,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'women', 'the', 'number', 'is', '[MASK]', 'in', 'forty', 'and', 'the', 'n', '##ns', 'are', 'more', 'likely', 'to', 'be', 'prison', 'staff', 'members', '.', 'SEP']\n",
      "['[CLS]', 'n', '##n', 'link', 'is', '[MASK]', 'the', 'company', 'uses', 'n', '##n', 'products', '.', 'SEP']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:01,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'support', 'he', 'is', '[MASK]', 'fine', 'editor', ',', 'but', 'has', 'too', 'little', 'edit', '##s', '.', 'SEP']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:01,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'method', 'keeps', '[MASK]', 'magnetic', 'n', '##n', 'that', 'the', 'n', '##n', 'experiences', ',', 'constant', 'over', 'the', 'n', '##n', \"'\", 's', 'normal', 'n', '##n', 'range', '.', 'SEP']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [00:02,  6.46it/s]\n"
     ]
    }
   ],
   "source": [
    "all_examples_x = []\n",
    "all_examples_vp = []\n",
    "all_examples_y = []\n",
    "\n",
    "all_examples_attentions = []\n",
    "all_examples_blankout_relevance = []\n",
    "all_examples_grads = []\n",
    "all_examples_inputgrads = []\n",
    "n_batches = 10\n",
    "\n",
    "all_examples_accuracies = []\n",
    "\n",
    "infl_eng = inflect.engine()\n",
    "verb_infl, noun_infl = gen_inflect_from_vocab(infl_eng, '../InDist/notebooks/wiki.vocab')\n",
    "\n",
    "test_data = task.databuilder.as_dataset(split='validation', batch_size=1)\n",
    "for examples in tqdm(test_data):\n",
    "    sentence = task.sentence_encoder().decode(examples['sentence'][0][1:])\n",
    "    if len(examples['sentence'][0][1:]) > 20:\n",
    "        continue\n",
    "    \n",
    "    verb_position = examples['verb_position'][0].numpy()  #+1 because of adding cls.\n",
    "    verb_position = offset_convertor(examples['sentence'][0], verb_position, task.sentence_encoder(), tokenizer)\n",
    "    \n",
    "    sentence = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    all_examples_vp.append(verb_position)\n",
    "    sentence[verb_position] = tokenizer.mask_token\n",
    "    \n",
    "    tf_input_ids = tokenizer.encode(sentence)\n",
    "    input_ids = torch.tensor([tf_input_ids])\n",
    "    \n",
    "    print(sentence)\n",
    "\n",
    "    \n",
    "    s_shape = input_ids.shape\n",
    "    batch_size, length = s_shape[0], s_shape[1]\n",
    "    actual_verb = examples['verb'][0].numpy().decode(\"utf-8\")\n",
    "    inflected_verb = verb_infl[actual_verb] \n",
    "\n",
    "\n",
    "    actual_verb_index = tokenizer.encode(tokenizer.tokenize(actual_verb))[1]\n",
    "    inflected_verb_index = tokenizer.encode(tokenizer.tokenize(inflected_verb))[1]\n",
    "\n",
    "    all_examples_x.append(input_ids)\n",
    "    embeded_inputs = torch.autograd.Variable(model.distilbert.embeddings(input_ids), requires_grad=True)\n",
    "    predictions = model(inputs_embeds=embeded_inputs)\n",
    "    logits = predictions[0][0]\n",
    "\n",
    "    \n",
    "        \n",
    "    probs = torch.nn.Softmax(dim=-1)(logits)\n",
    "    actual_verb_score = probs[verb_position][actual_verb_index]\n",
    "    inflected_verb_score = probs[verb_position][inflected_verb_index]\n",
    "    \n",
    "    main_diff_score = actual_verb_score - inflected_verb_score\n",
    "    \n",
    "    all_examples_accuracies.append(main_diff_score > 0)\n",
    "    \n",
    "    logits_sum = logits.sum()\n",
    "    logits_sum.backward()\n",
    "    grads = embeded_inputs.grad\n",
    "    grad_scores = abs(np.sum(grads.detach().numpy(), axis=-1))\n",
    "    input_grad_scores = abs(np.sum((grads * embeded_inputs).detach().numpy(), axis=-1))\n",
    "    all_examples_grads.append(grad_scores)\n",
    "    all_examples_inputgrads.append(input_grad_scores)\n",
    "    \n",
    "    hidden_states, attentions = predictions[-2:]\n",
    "    _attentions = [att.detach().numpy() for att in attentions]\n",
    "    attentions_mat = np.asarray(_attentions)[:,0]\n",
    "\n",
    "    all_examples_attentions.append(attentions_mat)\n",
    "    \n",
    "    n_batches -= 1\n",
    "    if n_batches <= 0:\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_att_relevance(full_att_mat, input_tokens, layer=-1, output_index=0):\n",
    "    raw_rel = full_att_mat[layer].sum(axis=0)[output_index]/full_att_mat[layer].sum(axis=0)[output_index].sum()\n",
    "    \n",
    "    return raw_rel\n",
    "\n",
    "\n",
    "def get_joint_relevance(full_att_mat, input_tokens, layer=-1, output_index=0):\n",
    "    att_sum_heads =  full_att_mat.sum(axis=1) / full_att_mat.shape[1]\n",
    "    joint_attentions = compute_joint_attention(att_sum_heads, add_residual=True)\n",
    "    relevance_attentions = joint_attentions[layer][output_index]\n",
    "    return relevance_attentions\n",
    "\n",
    "\n",
    "def get_flow_relevance(full_att_mat, input_tokens, layer, output_index):\n",
    "    \n",
    "    input_tokens = input_tokens\n",
    "    res_att_mat = full_att_mat.sum(axis=1)/full_att_mat.shape[1]\n",
    "    res_att_mat = res_att_mat + np.eye(res_att_mat.shape[1])[None,...]\n",
    "    res_att_mat = res_att_mat / res_att_mat.sum(axis=-1)[...,None]\n",
    "\n",
    "    res_adj_mat, res_labels_to_index = get_adjmat(mat=res_att_mat, input_tokens=input_tokens)\n",
    "    \n",
    "    A = res_adj_mat\n",
    "    res_G=nx.from_numpy_matrix(A, create_using=nx.DiGraph())\n",
    "    for i in np.arange(A.shape[0]):\n",
    "        for j in np.arange(A.shape[1]):\n",
    "            nx.set_edge_attributes(res_G, {(i,j): A[i,j]}, 'capacity')\n",
    "\n",
    "\n",
    "    output_nodes = ['L'+str(layer+1)+'_'+str(output_index)]\n",
    "    input_nodes = []\n",
    "    for key in res_labels_to_index:\n",
    "        if res_labels_to_index[key] < full_att_mat.shape[-1]:\n",
    "            input_nodes.append(key)\n",
    "    \n",
    "    flow_values = compute_node_flow(res_G, res_labels_to_index, input_nodes=input_nodes, output_nodes=output_nodes, length=full_att_mat.shape[-1])\n",
    "    \n",
    "    n_layers = full_att_mat.shape[0]\n",
    "    length = full_att_mat.shape[-1]\n",
    "    final_layer_attention = flow_values[(layer+1)*length:, layer*length:(layer+1)*length]\n",
    "    relevance_attention_flow = final_layer_attention[output_index]\n",
    "\n",
    "    return relevance_attention_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 12, 23, 23)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_examples_attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 7679.06it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 8284.23it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 8180.82it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 8328.64it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 9756.46it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 8649.83it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 5214.20it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 5457.07it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 5678.72it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 5382.83it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 5482.03it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 5225.24it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute raw relevance scores ...\n",
      "compute joint relevance scores ...\n",
      "compute flow relevance scores ...\n",
      "23 ['[CLS]', '[CLS]', 'ready', 'to', 'serve', 'at', 'every', 'opportunity', ',', 'yet', 'making', 'sure', 'that', 'your', 'fellow', 'servers', 'have', '[MASK]', 'equal', 'chance', '.', '[UNK]', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 1/10 [00:00<00:04,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 ['[CLS]', '[CLS]', 'reviewed', 'journals', 'are', '[MASK]', 'varying', 'degrees', 'of', 'reliability', '.', '[UNK]', '[SEP]']\n",
      "20 ['[CLS]', '[CLS]', 'operation', 'since', '1871', ',', 'the', 'network', 'is', '[MASK]', 'about', 'long', ',', 'and', 'comprises', '10', 'lines', '.', '[UNK]', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:00<00:02,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 ['[CLS]', '[CLS]', 'peak', 'times', ',', 'the', 'n', '##np', 'route', 'continues', '[MASK]', 'the', 'n', '##np', 'guided', 'n', '##np', 'to', 'cambridge', '.', '[UNK]', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:01<00:02,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 ['[CLS]', '[CLS]', 'first', 'requirement', '(', 'n', '##np', ')', 'simply', 'requires', '[MASK]', 'a', 'cd', 'should', 'be', 'a', 'distribution', 'on', 'the', 'parameter', 'space', '.', '[UNK]', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:02<00:01,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ['[CLS]', '[CLS]', 'letters', 'are', '[MASK]', 'and', 'jj', '.', '[UNK]', '[SEP]']\n",
      "24 ['[CLS]', '[CLS]', 'women', 'the', 'number', 'is', '[MASK]', 'in', 'forty', 'and', 'the', 'n', '##ns', 'are', 'more', 'likely', 'to', 'be', 'prison', 'staff', 'members', '.', '[UNK]', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [00:02<00:00,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 ['[CLS]', '[CLS]', 'n', '##n', 'link', 'is', '[MASK]', 'the', 'company', 'uses', 'n', '##n', 'products', '.', '[UNK]', '[SEP]']\n",
      "18 ['[CLS]', '[CLS]', 'support', 'he', 'is', '[MASK]', 'fine', 'editor', ',', 'but', 'has', 'too', 'little', 'edit', '##s', '.', '[UNK]', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [00:02<00:00,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 ['[CLS]', '[CLS]', 'method', 'keeps', '[MASK]', 'magnetic', 'n', '##n', 'that', 'the', 'n', '##n', 'experiences', ',', 'constant', 'over', 'the', 'n', '##n', \"'\", 's', 'normal', 'n', '##n', 'range', '.', '[UNK]', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:03<00:00,  2.62it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 ['[CLS]', '[CLS]', 'ready', 'to', 'serve', 'at', 'every', 'opportunity', ',', 'yet', 'making', 'sure', 'that', 'your', 'fellow', 'servers', 'have', '[MASK]', 'equal', 'chance', '.', '[UNK]', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 1/10 [00:00<00:03,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 ['[CLS]', '[CLS]', 'reviewed', 'journals', 'are', '[MASK]', 'varying', 'degrees', 'of', 'reliability', '.', '[UNK]', '[SEP]']\n",
      "20 ['[CLS]', '[CLS]', 'operation', 'since', '1871', ',', 'the', 'network', 'is', '[MASK]', 'about', 'long', ',', 'and', 'comprises', '10', 'lines', '.', '[UNK]', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:00<00:02,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 ['[CLS]', '[CLS]', 'peak', 'times', ',', 'the', 'n', '##np', 'route', 'continues', '[MASK]', 'the', 'n', '##np', 'guided', 'n', '##np', 'to', 'cambridge', '.', '[UNK]', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:01<00:02,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 ['[CLS]', '[CLS]', 'first', 'requirement', '(', 'n', '##np', ')', 'simply', 'requires', '[MASK]', 'a', 'cd', 'should', 'be', 'a', 'distribution', 'on', 'the', 'parameter', 'space', '.', '[UNK]', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:02<00:02,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ['[CLS]', '[CLS]', 'letters', 'are', '[MASK]', 'and', 'jj', '.', '[UNK]', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-81ad61c7aa1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mattention_relevance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_flow_relevance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_examples_attentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mall_examples_flow_relevance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_relevance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-132d7925c0b9>\u001b[0m in \u001b[0;36mget_flow_relevance\u001b[0;34m(full_att_mat, input_tokens, layer, output_index)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0minput_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mflow_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_node_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_G\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_labels_to_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_att_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mn_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_att_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Codes/attention_flow/attention_graph_util.py\u001b[0m in \u001b[0;36mcompute_node_flow\u001b[0;34m(G, labels_to_index, input_nodes, output_nodes, length)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minp_node_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minp_node_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0mflow_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum_flow_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medmonds_karp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0mflow_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpre_layer\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflow_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mflow_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mflow_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indist/lib/python3.7/site-packages/networkx/algorithms/flow/maxflow.py\u001b[0m in \u001b[0;36mmaximum_flow_value\u001b[0;34m(flowG, _s, _t, capacity, flow_func, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNetworkXError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"flow_func has to be callable.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflow_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflowG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapacity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcapacity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'flow_value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indist/lib/python3.7/site-packages/networkx/algorithms/flow/edmondskarp.py\u001b[0m in \u001b[0;36medmonds_karp\u001b[0;34m(G, s, t, capacity, residual, value_only, cutoff)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medmonds_karp_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapacity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0mR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'algorithm'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'edmonds_karp'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indist/lib/python3.7/site-packages/networkx/algorithms/flow/edmondskarp.py\u001b[0m in \u001b[0;36medmonds_karp_impl\u001b[0;34m(G, s, t, capacity, residual, cutoff)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcutoff\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mcutoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'flow_value'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medmonds_karp_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indist/lib/python3.7/site-packages/networkx/algorithms/flow/edmondskarp.py\u001b[0m in \u001b[0;36medmonds_karp_core\u001b[0;34m(R, s, t, cutoff)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mflow_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mflow_value\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msucc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbidirectional_bfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/indist/lib/python3.7/site-packages/networkx/algorithms/flow/edmondskarp.py\u001b[0m in \u001b[0;36mbidirectional_bfs\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mR_succ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'flow'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'capacity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                             \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msucc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                                 \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msucc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"compute raw relevance scores ...\")\n",
    "all_examples_raw_relevance = {}\n",
    "for l in np.arange(0,6):\n",
    "    all_examples_raw_relevance[l] = []\n",
    "    for i in tqdm(np.arange(len(all_examples_x))):\n",
    "        tokens = tokenizer.decode(all_examples_x[i][0].numpy())\n",
    "        vp = all_examples_vp[i]\n",
    "        length = len(tokens)\n",
    "        attention_relevance = get_raw_att_relevance(all_examples_attentions[i], tokens, layer=l, output_index=vp)\n",
    "        all_examples_raw_relevance[l].append(np.asarray(attention_relevance))\n",
    "\n",
    "print(\"compute joint relevance scores ...\")\n",
    "all_examples_joint_relevance = {}\n",
    "for l in np.arange(0,6):\n",
    "    all_examples_joint_relevance[l] = []\n",
    "    for i in tqdm(np.arange(len(all_examples_x))):\n",
    "        tokens = tokenizer.decode(all_examples_x[i][0].numpy())\n",
    "        vp = all_examples_vp[i]\n",
    "        length = len(tokens)\n",
    "        attention_relevance = get_joint_relevance(all_examples_attentions[i], tokens, layer=l, output_index=vp)\n",
    "        all_examples_joint_relevance[l].append(np.asarray(attention_relevance))\n",
    "    \n",
    "print(\"compute flow relevance scores ...\")\n",
    "all_examples_flow_relevance = {}\n",
    "for l in np.arange(0,6):\n",
    "    all_examples_flow_relevance[l] = []\n",
    "    for i in tqdm(np.arange(len(all_examples_x))):\n",
    "        tokens = tokenizer.tokenize(tokenizer.decode(all_examples_x[i][0].numpy()))\n",
    "        vp = all_examples_vp[i]\n",
    "        length = len(tokens)\n",
    "        print(length, tokens)\n",
    "        attention_relevance = get_flow_relevance(all_examples_attentions[i], tokens, layer=l, output_index=vp)\n",
    "        all_examples_flow_relevance[l].append(np.asarray(attention_relevance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############Layer  0 #############\n",
      "raw grad\n",
      "(23,) (23,)\n",
      "0.3125820001288273 0.162645819527672\n",
      "joint grad\n",
      "(23,) (23,)\n",
      "0.2563954068365207 0.11079837409268715\n",
      "flow grad\n",
      "(23,) (23,)\n",
      "0.2563954068365207\n",
      "###############Layer  1 #############\n",
      "raw grad\n",
      "(23,) (23,)\n",
      "0.14869072355150909 0.21877963951550075\n",
      "joint grad\n",
      "(23,) (23,)\n",
      "0.16916921869467577 0.13068662468721445\n",
      "flow grad\n",
      "(23,) (23,)\n",
      "0.1710263273059909\n",
      "###############Layer  2 #############\n",
      "raw grad\n",
      "(23,) (23,)\n",
      "0.1267169586483366 0.22054674718995168\n",
      "joint grad\n",
      "(23,) (23,)\n",
      "0.14307649063756306 0.1368156884633754\n",
      "flow grad\n",
      "(23,) (23,)\n",
      "0.16480121835871206\n",
      "###############Layer  3 #############\n",
      "raw grad\n",
      "(23,) (23,)\n",
      "0.08168881311084977 0.23628447400788619\n",
      "joint grad\n",
      "(23,) (23,)\n",
      "0.11879433314066115 0.14483665860350683\n",
      "flow grad\n",
      "(23,) (23,)\n",
      "0.15879912601482044\n",
      "###############Layer  4 #############\n",
      "raw grad\n",
      "(23,) (23,)\n",
      "0.07231885501893161 0.23114840345337834\n",
      "joint grad\n",
      "(23,) (23,)\n",
      "0.09436668150647383 0.1492732124745169\n",
      "flow grad\n",
      "(23,) (23,)\n",
      "0.17919749203607835\n",
      "###############Layer  5 #############\n",
      "raw grad\n",
      "(23,) (23,)\n",
      "0.02348991874869261 0.24934316926216385\n",
      "joint grad\n",
      "(23,) (23,)\n",
      "0.06976944776904395 0.1604885773344236\n",
      "flow grad\n",
      "(23,) (23,)\n",
      "0.19945231513164136\n"
     ]
    }
   ],
   "source": [
    "raw_sps_blank = []\n",
    "raw_sps_grad = []\n",
    "raw_sps_inputgrad = []\n",
    "\n",
    "joint_sps_blank = []\n",
    "joint_sps_grad = []\n",
    "joint_sps_inputgrad = []\n",
    "\n",
    "flow_sps_blank = []\n",
    "flow_sps_grad = []\n",
    "flow_sps_inputgrad = []\n",
    "\n",
    "\n",
    "for l in np.arange(0,6):\n",
    "    print(\"###############Layer \",l, \"#############\")\n",
    "\n",
    "    print('raw grad')\n",
    "    print(all_examples_raw_relevance[l][0].shape, all_examples_grads[0][0].shape)\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = spearmanr(all_examples_raw_relevance[l][i],all_examples_grads[i][0])\n",
    "        if not math.isnan(sp[0]):\n",
    "            raw_sps_grad.append(sp[0])\n",
    "        else:\n",
    "            raw_sps_grad.append(0)\n",
    "        \n",
    "    print(np.mean(raw_sps_grad), np.std(raw_sps_grad))\n",
    "\n",
    "    \n",
    "    print('joint grad')\n",
    "    print(all_examples_joint_relevance[l][0].shape, all_examples_grads[0][0].shape)\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = spearmanr(all_examples_joint_relevance[l][i],all_examples_grads[i][0])\n",
    "        if not math.isnan(sp[0]):\n",
    "            joint_sps_grad.append(sp[0])\n",
    "        else:\n",
    "            joint_sps_grad.append(0)\n",
    "        \n",
    "    print(np.mean(joint_sps_grad), np.std(joint_sps_grad))\n",
    "\n",
    "  \n",
    "    print('flow grad')\n",
    "    print(all_examples_joint_relevance[l][0].shape, all_examples_grads[0][0].shape)\n",
    "    for i in np.arange(len(all_examples_x)):\n",
    "        sp = spearmanr(all_examples_flow_relevance[l][i],all_examples_grads[i][0])\n",
    "        if not math.isnan(sp[0]):\n",
    "            flow_sps_grad.append(sp[0])\n",
    "        else:\n",
    "            flow_sps_grad.append(0)\n",
    "        \n",
    "    print(np.mean(flow_sps_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_examples_joint_relevance[l][0].shape)\n",
    "print(all_examples_flow_relevance[l][0].shape)\n",
    "print(all_examples_blankout_relevance[0].numpy().shape)\n",
    "print(all_examples_inputgrads[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_examples_inputgrads[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = model\n",
    "\n",
    "sentences = []\n",
    "all_atts = []\n",
    "all_main_probs = []\n",
    "all_index_probs = []\n",
    "all_gradient_scores = []\n",
    "all_inputgradient_scores = []\n",
    "prob_fn = task.get_probs_fn()\n",
    "count = 0\n",
    "for x, y in task.test_dataset:\n",
    "    \n",
    "     #Manually add cls token:\n",
    "    batch_size = len(x)\n",
    "    cl_token = tf.reshape(tf.convert_to_tensor(cl_token[0], dtype=tf.int64)[None], (-1,1))\n",
    "    cl_tokens = tf.tile(cl_token, (batch_size, 1))\n",
    "    x = tf.concat([cl_tokens, x], axis=-1)\n",
    "    \n",
    "    # Get gradient scores \n",
    "    input_embeddings, input_shape, padding_mask, past = model.get_input_embeddings(x, training=False, add_cls=False)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_embeddings)\n",
    "        outputs = model_1.call_with_embeddings(input_embeddings, input_shape, padding_mask, past)\n",
    "        logits = outputs[0]\n",
    "        probs = tf.nn.softmax(logits, axis=-1)\n",
    "        diff_probs = probs[:,0] - probs[:,1]\n",
    "        \n",
    "    grads = tape.gradient(diff_probs, input_embeddings)\n",
    "    grad_scores = tf.abs(tf.reduce_sum(grads, axis=-1))\n",
    "    input_grad_scores = tf.abs(tf.reduce_sum(tf.multiply(grads, input_embeddings), axis=-1))\n",
    "    \n",
    "    \n",
    "    all_gradient_scores.extend(grad_scores)\n",
    "    all_inputgradient_scores.extend(input_grad_scores)\n",
    "    \n",
    "    \n",
    "    max_len = x.shape[1]\n",
    "    all_outputs = model_1.detailed_call(x, training=False, add_cls=False)\n",
    "    main_logits = all_outputs[0]\n",
    "    attentions = all_outputs[6]\n",
    "    _attentions = [att.numpy() for att in attentions]\n",
    "    attentions = np.transpose(np.asarray(_attentions), (1,0,2,3,4))\n",
    "    main_probs = prob_fn(main_logits, y, 1)\n",
    "    batch_indexes = tf.range(len(y), dtype=tf.int64)\n",
    "    indexes = tf.concat([batch_indexes[:,None], y[:,None]], axis=1)\n",
    "    correct_main_probs = tf.gather_nd(main_probs, indexes).numpy()\n",
    "\n",
    "    sentences.append(task.databuilder.sentence_encoder().decode(x[0]))\n",
    "    all_atts.extend(attentions)\n",
    "    all_main_probs.extend(correct_main_probs)\n",
    "    all_index_probs.append([])\n",
    "    \n",
    "    # This loop can be optimized so that there is only one call...\n",
    "    new_xz = []\n",
    "    for i in np.arange(0,max_len):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        unktoken = task.databuilder.sentence_encoder().encode(constants.unk)\n",
    "        unk = tf.reshape(tf.convert_to_tensor(unktoken, dtype=tf.int64)[None], (-1,1))\n",
    "        unks = tf.tile(unk, (batch_size, 1))\n",
    "        new_x = tf.concat([x[:,:i], unks, x[:,i+1:]], axis=-1)\n",
    "        new_xz.extend(new_x)\n",
    "    \n",
    "    new_x = np.asarray(new_xz)\n",
    "    logits = model_1(new_x, training=False, add_cls=False)\n",
    "    probs = prob_fn(logits, y, 1)\n",
    "    \n",
    "    batch_indexes = tf.range(len(probs), dtype=tf.int64)\n",
    "    yz = tf.tile(y, (len(probs),))\n",
    "\n",
    "    indexes = tf.concat([batch_indexes[:,None], yz[:,None]], axis=1)\n",
    "    \n",
    "    correct_probs = tf.gather_nd(probs, indexes).numpy()\n",
    "    all_index_probs[-1].extend(abs(correct_main_probs - correct_probs))\n",
    "    count += 1\n",
    "    if count > 100:\n",
    "        break\n",
    "    print (count, end=\"\\r\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
